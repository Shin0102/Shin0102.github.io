[{"content":"들어가며 5주차 강의에 접어들었다. 이번장에서는 MLP의 전반적인 과정에 대한 내용이었고, 그중에서 Forward Pass, Backward Pass에 대한 내용이 주요 내용이었다. 그리고 최적의 모델 성능을 위한 Optimizer 와, 학습속도와 안정도를 위해 사용하는 Batch Normalization 내용도 포함 되었다. 가장 멘붕오는 강의였던 것 같다. 같은 피어그룹 분들도 모두 동의하였다. 그래도 공유된 유튜브 강의나 블로그등을 보면서 조금이나마 이해도를 높이는데 도움이 된 것 같다.\n깊이보다는 AI에 대한 전반적인 지식을 기록한 글입니다.\n5주차 Backward Pass Optimizer 외우지 않고 배우는 모델 Batch Normalization (실습) MLP MNIST classification (2) Backward Pass Back Propagation 이란? loss를 줄이는 방향으로 각 parameter를 조절하기위해 각 parameter에 대한 loss의 편미분 값을 계산하여 이를 이용해 parameter를 update 편미분(Partial Derivative): MLP와 같은 다변수 함수는 각 변수들이 복합적으로 함수에 영향을 주기 때문에 다른 변수들의 값을 상수로 둔 상태에서 특정 변수에 대한 도함수 값을 고려 Chain Rule: 연쇄 법칙을 활용하여, Forward Pass 시 저장하였던 Local Gradient와 Back Propagation으로 역으로 내려온 Global Gradient 값을 이용하여 전체 Loss 에 편미분 값을 구할수 있다. Forward Pass - Backward Pass MLP의 연산들은 matrix multiplication과 nonlinear activation function 으로 구성되어 아래와 같이 구분가능\nForward Pass 들의 basic operation 4가지를 보고, Backward Pass를 할때 에러 시그널이 어떻게 전달되는지 살펴보자.\n1: Addition Operation 2: Multiplication Operation 3: Common Variable Operation 4: Nonlinear Active Fucnction Nonlinear Activation Function\nActivation function의 backward pass를 위해서는 해당 node에서 각 함수의 미분값이 필요.\nSigmoid, ReLu \u0026hellip; Optimizer gradient descent를 통해서 loss에 대해서 최적 parameter 값을 구하기 위해서 여러번 업데이트를 진행한다. Deep Learning 에서는 parameter space는 차원이 굉장히 크다. -\u0026gt; global optimal point 찾는 것이 불가능. saddle point(말 안장의 모양)를 피하고 local minima를 찾는데 목표로 한다. Gradient-based Methods First-order Optimization Methods Parameter를 loss function gradient의 반대방향으로 update 하여 loss function이 더 작은 paramter를 얻음 mini-batch를 이용 Stocahstic Gradient Descent SGD\nParameter를 gradient 반대 방향으로 update 가장빠르고 쉽게 적용가능하나 saddle points에 빠지기 쉬움 Gradeint에 noise가 많이 발생, update의 방향이 진동하기 쉬움 Momentum\ngradeint가 빠르게 변하는 것을 막으며 일관된 방향으로 update 유도 Hyper-parameter momentum factor가 추가 됨 AdaGrad\nUpdate 방향이 과하게 진동하는 문제를 해결하기 위해 prameter-wise update history를 통해 parameter-wise learning rate 적용 Update 양이 많은 parameter의 update를 줄이고, update가 많이 진행되지 않은 parameter의 update를 늘림 learning rate가 계속 감소하여, Deap Learning에서 사용하기 어렵다. RMSprop\nAdaGrad의 gradient accumulation S의 momentum을 적용 너무 먼 과거의 gradient의 효과를 줄임 Adam\n가장 많이 사용됨 RMSprop과 momentum의 조합 Bias correction이라는 기법을 통해 각 momentum이 초반에 불안정하게 작동하는 것을 방지\nLearning Rate Scheduling\n학습이 직행될수록 parameter가 최적 값으로 다가가기 때문에 learning rate를 줄여 더 정확한 수렴을 시도 Linear decay, step decay, exponential decay Parameter Initialization\n초기 parameter 설정도 중요하다. 외우지 않고 배우는 모델 Regularization\nOverfitting을 막기 위한 기법 Overfitting: 데이터의 존재하는 noise까지 학습함에 따라 학습 데이터가 아닌 데이터에 대해 정확한 추론을 하지 못하는 경우 Norm Regularizations Early Stopping Validation set의 성능향상이 더 이상 나타나지 않을때 학습을 멈추는 기법 하지만 실제 학습시 validation 성능이 한참 오르지 않을때가 있기 때문에 주의가 필요 Ensemble Methods 다양한 hyper-parmeter 조절 + randomness Dropout 매번 forward pass를 할 때마다 전체 parameter 중 일부를 masking 모델 전체 parameter 중 일부를 이용해서도 좋은 성능을 얻을수 있도록 유도 batch normalizaion? 이 비슷한 효과를 낸다 Batch Normalization 학습안정도, 학습속도에 많은 개선을 준 알고리즘\nActivation Distribution Assumption\n모델 자체에 대한 분석과 여러 유용한 알고리즘 대부분 activation과 parameter 분포에 대해 Gaussian을 가정 실제로는 이 Gaussian 분포를 따르지 않는다. -\u0026gt; 학습속도가 느려지고 학습방향이 일정하지 않다. 이를 위해 mini-batch 단위로 activation은 normalize 하여 원하는 분포로 만들어준다. Batch Normalizaition\n각 layer의 activation을 batch 단위로 normalize를 하여 원하는 분포로 만들어 줌 RNA나 lstm의 경우 모델의 특성상 사용이 어렵다. 학습 과정에서는 mini-batch 전체의 정보를 이용해 batch-statics를 계산하여 normalize에 사용 ","description":"","id":0,"section":"posts","tags":["TIL","AI","Backward PAss","Optimizer","Batch Normalization"],"title":"Backward Pass, Optimizer, Batch Normalization","uri":"https://Shin0102.github.io/posts/socar-ai-bootcamp-til-5/"},{"content":"들어가며 4주차 강의에 접어들었다. 이제까지는 주로 Machine Learning 의 대한 내용들에 대해 다루었다. 강의의 진도가 매우 빠르다. TIL과 실습과제들을 해보면서 감을 잡고있긴한데 벅찬 것 같다. 피어그룹을 하면서 서로 진도체크 하는것이 도움이 되는 것 같다. 그리고 부트 캠프 전 기수중에 AI 엔지니어로 커리어 전환하신 분의 1시간 정도 강연을 해주셨는데, 그분도 처음에는 멘붕상태였다고 그랬다. 하지만 이해 안되는 것에 너무 집착하지 말고 최대한 강의와 과제를 따라가 결국엔 최우수 수강생이 되셨다고 하였다. 그렇게 위로아닌 위로를 받고 이제부터는 Deep Learning 에 대한 강의가 시작되는데 다시 한번 힘을 내야겠다.\n깊이보다는 AI에 대한 전반적인 지식을 기록한 글입니다. 4주차 Multi-Layer Perception Deep Learning Forward Pass Activation Function Loss Fucntion (실습) Pytorch Tutorial (실습) MLP MNIST Classfication Multi-Layer Perception Perceptron?\n다차원 입력 벡터에 가중치(w)를 곱해 출력 값을 얻는 알고리즘, 벡터간의 내적 + bias 선형 모델의 확장, 기존 선형모델이 해결하기 어려운 문제를 해결하기 위한 모델(XOR gate problem)\nMLP 구조\nParameters: Weight, Bias Activation Function: input과 output 관계에서 non-linearity(비선형성)을 준다. -\u0026gt; 좀 더 복잡한 문제를 해결 Loss Function MLP 동작 방식: Forward Pass(Parmeters 와 activation function을 이용) -\u0026gt; get Loss -\u0026gt; Backward Pass\nMLP 의 layer가 늘어나면 Parameter 수가 엄청나게 늘어나게 됨\nForward Pass 입력이 주어졌을 때 parameter와 activation function을 통해 출력은 추론하는 과정\nBatch Training: 학습이나 추론을 할때, 하나의 데이터가 아닌 여러개의 데이터를 묶어서 진행\n데이터의 묶음을 batch라 한다. 데이터를 여러개로 학습을 하게되면 좀 더 효율적으로 성능을 높일수있다. 행렬과 행렬의 곱으로 확장 Matrix Multiplication(행렬곱)?\nMini Batch Training:\n모든 데이터를 가지고 하는것은 메모리나 over fitting의 문제가 있음 효율적인 학습을 위해 random 하게 데이터를 sampling 1 epoch =\u0026gt; 여러개의 mini-batch를 통한 학습 epoch을 반복하면서 모델의 성능을 높인다. Activation Function Why? 각 layer의 연산은 선형 연산, 비선형 성질을 가진 activation function을 적용하여 모델이 더 다양한 표현력을 가지도록 함 Activation Function 종류 Sigmoid: 미분 계수?의 계산이 간단, 미분 계수의 값이 0이 되는 영역이 너무 넓음, Vanishing Gradient Problem tanh: Vanishing Gradient Problem ReLu: input value의 max operation(max(0,x)), 계산이 굉장히 빠름, 주로 많이 사용됨 Leaky-Relu: ReLu의 값이 음수에서 사라지는 문제를 해결 Softmax Function 모델 output 부분에서 사용 모델이 어떤 class로 추정했는지 이해하는데 도움을 줌 Loss Function 모델의 output이 얼마나 틀렸는지를 나타내는 척도, 나중에 backward pass에서 모델의 parameter를 수정하는데 사용 주로 regression task는 MSE loss function을 사용 주로 classification task는 cross-entropy loss function을 사용 ","description":"","id":1,"section":"posts","tags":["TIL","AI","Mult-Layer Perception","Deep Learning","Forward Pass","Activation Function","Loss Function"],"title":"Multi-Layer Perceptron","uri":"https://Shin0102.github.io/posts/socar-ai-bootcamp-til-4/"},{"content":"들어가며 3주차 강의에 접어들었다. 이번 강의는 Machine Learning중 output이 주어지지 않는 Unsupervised Learning에 대한 내용이다. 생소한 수식들과 처음듣는 용어들 때문에 이해하기가 힘들지만, 너무 집착하지말고 최대한 숲을 보려고 노력중이다. 그래도 실습에서 실제 데이터를 가지고 진행하다보니 좀 더 이해하기가 수월하였다. 그리고 이번 부트캠프에서는 Peer Group을 정해주는데, 아무래도 온라인으로 진행하다보니 서로의 진도를 체크하고, Study에 있어 서로에게 도움을 주는 것이 목적인것 같았다. 아무래도 취준생이나 학생분들이 많이 참여한 Group들이 많이 보였다. 다행히도 직장인분들이 많은 Group이 있어 그곳에 참여하여 만남을 진행하였고, 다양한 분야의 사람들을 만나보고 같이 Study 하는 기회를 가지게되어 좋았던 것 같다.\n깊이보다는 AI에 대한 전반적인 지식을 기록한 글입니다.\n3주차 Unsupervised Learning Dimensionallity Reduction Clustering (실습) Data Analysis with Pandas Unsupervised Learning 정답 label이 없는 트레이닝셋이 주어짐 output을 예측을 하는것이 목표가 아니라, input feature에서 의미있는 패턴 찾기가 목적이다. 시각화, 전처리, 차원축소등의 데이터 분석 ,이상 탐지등의 목적으로 사용 Dimensionallity Reduction, Clustering Dimensionallity Reduction High-dimensional Data: 추천시스템(users * movies), 이미지, 동영상, 유전자 분석\nCurse of dimensionality: 데이터가 고차원일수록,같은 성능의 모델 학습을 위해 많은 데이터가 필요\n불필요하게 중복되는 변수나 의미없는 변수를 줄이자\nPCA(Principal Compo): 데이터 variance를 보존하면서 차원축소\n데이터의 분산을 가장 잘 설명해주는 축을 찾는다. projection 이후 variance를 최대화하는 축 → Convariance matrix? 를 최대로하는 PVE? ㄴ Scree plot에서 “elobw point”를 찾거나, 미리 정한 크기의 분산을 설명하는 가장 작은 components를 사용 한계점: classificaion에 도움이 되지않을 수 있다.(variance에 초점을 맞추기때문) MDS: 데이터간의 거리를 보존하면서 차원축소\nt-SNE: local neighborhood 정보를 보존하면서 차원축소, 차원에서 멀리 떨어져있는 데이터는 신경을 별로 쓰지않고 가까이 있는 데이터들이 차원축소 후에도 가까워져 있기를 기대한다.\n데이터가 주어졌을때 neighbor일 확률은 gaussian 분포를 따른다. Auto-encoder, Word2Vec: 딥러닝 기반의 차원 축소\nClustering 문서, 이미지 군집화, 주식 종목 군집화, 상권 분석, 구매 패턴등\nPartitioning Clustering: 사전에 정의된 숫자의 군집중 하나에 소속\nK-Means Clustering: 각 군집은 하나의 중심을 가짐(centroid) 사전에 군집의 수 K가 정해져야 함 SSE를 최소화 하는 partition을 찾는것 \u0026gt; elbow point k를 찾는것, 그 이상은 overfitting 한계점: 군집의 크기, 밀도가 다르거나 구형이 아닌경우 좋지 않은 결과가 나옴\nHierarchial Clustering: 계층적인 데이터 군집화. Dendrogram\nAgglomerative Clustering: K를 미리 정해줄 필요가 없음 거리 계산방식에 따라 다양한 결합 (linkage) 방식이 있음 min distance, max distnace, average distance, centroid distance 한계점: 계산복잡도가 크다, 군집화가 잘못되면 되돌릴 수 없다.\nDensity-based Clustering: 데이터의 density를 기반으로 임의의 형태의 군집을 찾는것\nDBSCAN: 데이터의 densitiy가 높은 영역과 그렇지 않은 영역으로 구분 ","description":"","id":2,"section":"posts","tags":["TIL","AI","Unsupervised Learning","Dimensionallity Reduction","Clustering"],"title":"Unsupervised Learning - Dimensionallity Reduction, Clustering","uri":"https://Shin0102.github.io/posts/socar-ai-bootcamp-til-3/"},{"content":"들어가며 2주차 강의에 접어들었다. 이번 강의는 Machine Learning중 output이 주어지는 Supervised Learning에 대해서, 그리고 기본이 되는 Linear Model에 대한 내용을 기반으로 Decision Tree, Ensemble 기법에 대한 내용 이었다. 이제 본격적으로 강의가 시작된 것인데, 엄청 깊게 들어가지는 않지만 아직 용어나 개념에 익숙치 않다보니 쉽지 않은 내용이었다. 잡설은 그만하고 강의 내용 정리해보자.\n깊이보다는 AI에 대한 전반적인 지식을 기록한 글입니다.\n2주차 Supervised Learning Linear Model Decision Tree Ensemble (실습) Logistic Regression (실습) Decision Tree Supervised Learning 지도 학습, 정답 레이블이 있는 학습방법, input x에 대해서 output 을 예측하는 방법이다. Train 단계에서 모델을 fitting 하고, Test 단계에서 fitting 한 모델 inference(추론)를 진행한다.\ny = f(x) (input x에 대한 y값을 추론)\nSupervised Learning에 다양한 예시들 Linear Model: 다른 복잡한 모델의 가장 기본이 되는 모델.\nLinear Regression\nLogistic Regression: decision boundary를 찾는 알고리즘, 데이터를 분류할때 사용한다.\nSupport Vector Machine(SVM): decision boundary(Maximum margin seperator)를 찾는 알고리즘\nMaximum Margin Seperator: Seperator(선)를 기준으로 양옆으로 데이터 margin이 가장 먼 Seperator Support Vector: Maximum Margin Seperator와 가장 접해있는 데이터\nNaive Bayes Classification: 확률 기반 model, Bayes Rule 이라는 확률공식을 사용, 정확도 보다는 가볍고 빠르게 돌아야하는 어플리케이션에서 사용. (ex. spam 필터링, 문장 감정 분석, 추천 시스템)\nGaussian Process: 확률 기반 model, 데이터 {x, f(x)}가 Multivariate(다변수) Gaussian 분포라 가정, 예측에 대한 confidence를 알 수 있음\nK-Nearest Neighbors(KNN):\nNonparametric approach (Training 데이터가 늘어나면 paramter 개수도 늘어나는 형태) 트레이닝 데이터를 모두 저장해 놨다가, 새로운 input 데이터가 들어왔을때 가장 가까운 k개의 값들을 통해 output을 알아낸다. Curse of dimensionality: input feature가 고차원일때 많은 양의 학습데이터 필요하다.\nDecision Tree: Explainable 하다. 사람의 사고방식과 유사. Overfitting이 되기 쉽다.\nRandom Forest: 여러개의 Decision Tree의 Ensemble, 사람들의 집단지성과 같은 느낌이다. Variance가 감소\nNeural Network\n어떤 상황에도 잘맞는 알고리즘은 없다. 데이터의 형태나 개수 컴퓨팅 자원등을 고려하여 적절한 알고리즘을 사용하여야 한다.\nLinear Model 다른 복잡한 모델의 기본이 되는 모델이다. 간단한 모델 -\u0026gt; Generalization 확장성: 다른 복잡한 모델의 기본이 되는 모델 Linear Regression y = wx + b loss(오차)으로는 MSE(제곱의 평균) function을 사용 loss를 최소화 하기위한 w, b를 찾는다. Gradient Descent(경사하강법): 2차 함수의 기울기가 0이 되는것을 찾는것이 목적 closed-from solution, gradient descent general linear model: nonlinear 한 데이터를 fitting Linear Classification linear decision boundary를 찾는 것이 목표 threshold function을 사용한 분류(0 or 1) Perceptron(신경망) Logistic Regression logistic 함수를 이용해서 class label이 1일 확률을 예측\nsoft threshold, sigmoid 라고도 불림 (non linearlity)\nMSE loss 대신 log loss 사용\nㄴ non linearlity 때문에\nㄴ MSE를 사용하면 loss function이 2차함수의 형태가 아니게 되므로\u0026hellip;\nDecision Tree 예측변수(Predictor): input feature 예측변수의 공간을 여러영역으로 계층화, 분할 Explainalbe 하다, 시각화하기 좋다 over fitting에 취약, input data의 작은 변화에도 예측값이 크게 변화 -\u0026gt; Ensemble 기법 Regression Tree 전체 예측변수 공간을 겹치지 않는영역으로 box들로 분할한다\n그곳에 속하는 training data 의 평균을 통해 예측값을 반환한다\nSum of Squared Errors (SSE)를 최소화하는 것이 목표\ntop-down, greedy: root 노드부터 단계별로 SSE가 최소화되는 split을 찾는다. (Recursive Binary Splitting)\nstopping criterion: over fitting을 방지하기 위해 leaf node의 데이터 수를 제한 하는것(ex. leaf node의 개수를 5개로 제한)\nPruning a Tree (큰 Tree를 만든후 Pruning(가지치기))\nㄴ Cost complexity pruning(weakest link pruning)\nClassification Tree 가장 많이 등장하는 class가 예측 class가 된다. Classification error rate: 가장 비율이 높은 클래스의 비율만 고려 Gini index, Entropy: 한 클래스로 분포가 치우치게 되면 작은값, 골고루 분포되면 큰값 -\u0026gt; 작은값 일수록 좋다. (node impurity라고도 부른다) Pruning에서는 최종예측 정확도를 위해 Classification error rate를 주로 사용 Ensemble Methods 여러 개의 간단한 \u0026lsquo;building block\u0026rsquo; 모델들을 결합해 하나의 강력한 모델을 만드는 법 Decision Tree -\u0026gt; Bagging, Random Forest, Boosting Bagging Bootstrap aggregation Variance 감소 효과 다른 tree 여러개의 결과를 평균해서 최종 예측 -\u0026gt; low variance Bootstrapping: 데이터셋에서 random한 복원추출을 통해 B개의 bootstrapped 데이터셋을 만드는 것 Out-of-Bag error estimation: bootstrapped 되지 않은 데이터를 validation error를 계산 Variable Importance Meassures: 모든 tree에 대한 예측변수의 split으로 인해 SSE 감소한 정도를 측정하여 평균을 취함. 예측 변수가 판단에 도움이 어느정도 되는지 파악하는데 도움이 된다. Random Forest tree들을 decorrelate(data set들간의 correlation을 줄이는 것) 해주고자 split을 진행할 때마다 전체 p개의 예측변수 중 랜덤하게 m개의 변수를 뽑고, 이들만 고려하여 split 진행 Boosting Bagging과 다르게 학습한 Tree들의 정보를 이용하여 순차적으로 Tree 학습\nBias 감소 효과\n전체 data set을 사용, 잘못 예측한 데이터에 집중하여 반복학습을 시킨다.\n3개의 hyperparameter\nTree의 개수 B: 너무 크면 overfit 될 수 있다. Shirinkage parameter: 학습 속도를 조절, 0.01 ~ 0.001 split 횟수 d: boosted tree의 complexity를 조절, boosting을 통해 bias를 줄이므로 d가 클 필요가 없다. Gradient Boosting, AdaBoost ","description":"","id":3,"section":"posts","tags":["TIL","AI","Supervised Learning","Linear Model","Decision Tree","Ensemble"],"title":"Supervised Learning - Linear Model, Decision Tree, Ensemble","uri":"https://Shin0102.github.io/posts/socar-ai-bootcamp-til-2/"},{"content":"들어가며 AI에 대해서 흥미는 있었지만, 막연하게 미디어에서만 접하다가 좀 더 깊이 있게 배워보고 싶다는 생각을 하던 중에 project lion이란 곳에서 AI 부트 캠프를 시작한다는 이야기를 듣고 덜컥 신청해버렸다. 이번년도에 뭔가 새로운 분야를 공부하면 좋겠다 생각을 했었는데, 좋은 기회가 될 것 같다. 총 8개의 챕터로 구성되어 있고 온라인으로 진행된다. 매주 한 챕터씩 강의가 오픈되며 중간중간 과제와 마지막 해커톤(5주)으로 총 13주로 마무리되는 일정이다. TIL을 통해 강의 내용을 정리하고 부족한 부분을 채워넣을 생각이다. 1주차는 AI의 전반적인 가벼운 소개로 시작되었고, 그 중 Machine Learning에 대해서 중점적으로 강의가 진행 되었다.\n깊이보다는 AI에 대한 전반적인 지식을 기록한 글입니다. 1주차 AI, 머신러닝, 딥러닝의 이해 머신러닝 basic 머신러닝 문제의 분류 학습방법의 분류 (실습) numpy AI \u0026gt; Machine Learning \u0026gt; Deep Learning AI, ML, DL의 관계는 위와같이 포함관계라고 한다. 아직은 용어나 알고리즘 이름들이 익숙치 않지만 정리해보면 아래와 같다.\n1. AI 사람처럼 행동하고 생각하고, 합리적으로 생각하고 행동한다. 이것을 컴퓨터로 구현한 기술 기존의 Traditional AI: Rule-based System, 분명한 한계점이 존재하여 Machine Learning의 등장하게 되었다. Traditional AI 종류에는 Search alg, Propositional Logic, First-Order Logic, Plannig 등이 있다. 2. Machine Learning 주어진 데이터로부터 학습을 하고, 새로운 정보를 얻어내거나 예측하는 기술. 데이터가 많을수록 성능이 좋아진다. Supervised Learning, Unsupervised Learning ex. Linear regression, Decision tree, K-means Clustering 3. Deep Learning neural network Hierarchial representation learning (계층적 학습) ex. visual / speech recognition 나 structured data에 활용(개인화 추천) Machine Learning ML의 프로세스는, Training 단계에서 training data를 통해서 모델을 학습하고, 학습한 모델을 통해서 Test 단계에서 test data를 통해 모델의 성능을 측정한다. ML Data set traing set(학습을 위한), validation set(모델을 선정하기 위한), test set(성능을 측정하기 위한) Data set이 충분히 크지 않다면? k-fold cross validation을 이용한다. k-fold cross validation: Data set을 K개의 부분집합으로 Data set을 구성하고 그 중 1개는 validation set 나머지는 training set으로 사용한다, 데이터수가 너무 적게되면 underfitting 되어 모델의 성능이 좋지 않을 수 있기때문에 사용한다. ML의 목적 새로운 input data가 들어오더라도 좋은 성능을 내는 모델을 학습 underfitting(high bias(편향)) -\u0026gt; optimization, more complex model 필요 overfitting(high variance(분산)) -\u0026gt; regularization, more data 필요 위의 under, overfitting을 피하기 위해서 적절한 bias-variance Trade off 가 필요하다.\n출처: http://scott.fortmann-roe.com ML 문제(task) 분류 Classifiation: 데이터를 label하고 새로운 데이터가 들어왔을때 labeling하는 것(분류) Regression: input 데이터에 대한 output 데이터를 맵핑 ex. Linear regression, Logisitic regression Densitiy Estimation: input 데이터에 확률분포(패턴)을 찾는 것 ML 학습 방법의 분류 Supervised Learning: input에 대한 output에 대한 값이 주어짐(labeled) ex. 알파고, siri, translator Unsupervised Learning: input에 대한 output이 아닌 의미있는 패턴을 찾는 것(unlabed) ex. Clustering, auto-encoder Reinforcement Learning: observation하고 action에 대해 reward값을 줌으로써 학습한다. Semi-supervised Learning Self-supervised Learning ","description":"","id":4,"section":"posts","tags":["TIL","AI","Machine Learning","Deep Learning"],"title":"AI, Machine Learning, Deep Learning?","uri":"https://Shin0102.github.io/posts/socar-ai-bootcamp-til-1/"},{"content":"들어가며 3년 넘게 다니던 회사를 그만두게 되고, 타이밍 좋게? 지인소개로 새로 런칭하는 서비스(재능기부 플랫폼)의 백엔드 개발에 참여하게 되었다. 레거시 없이 내가 원하는 백엔드 인프라를 구성할수 있다는 점과 초기멤버로 서비스 런칭한다는 점이 끌렸고, 새로운 도전을 하고싶어 입사하게 되었다. 입사당시 팀원은 대표, CTO, 나 세명이었고 추가적으로 백엔드 팀원 두명의 TO와 프론트엔드 외주가 예정되어 있었고, 서비스 기획이 어느정도 마무리 되는 시점이여서 첫 한달은 비즈니스를 이해하고 인프라 설계 및 API 설계하는데 주력하였다. 그렇게 어느정도 개발 윤곽이 나오고 \u0026lsquo;이제 달리기만 하면 된다\u0026rsquo; 생각했다. 하지만 살면서 계획대로 되는일이 얼마나 되겠는가\u0026hellip; 뭐 많은일들이 있었지만 결론적으로는 마크업과 앱 배포만 외주로 지원받고 나머지는 혼자 개발하기로 스스로 결정하였다. 기대반 두려움반 이렇게 1인개발 여정이 시작되었다. 🚴🏼🚴🏼🚴🏼\n험난한 여정 백엔드 구성은 어느정도 CTO님과 얘기하며 어느정도 윤곽이 나왔다. MSA로 구성하였으며 terraform을 이용하여 AWS EKS로 구축하기로 했다. 추후 스케일링 뿐만아니라 자동화 파이프라인을 구축하여 개발에 집중하기 좋을거라 생각했다. DDD(Domain Driven Design) 방법론을 이용하여 개발해야할 모듈이 4개로 정했으며, 원래는 JAVA Spring으로 진행하고 싶었지만 짧은기간(약 6개월)에 프론트엔드 개발도 동시에 진행해야 했기에 익숙한 Python Django로 진행하고 다른 작은모듈은 Python fastapi나 Node express를 사용하였다. 백엔드 파트는 그래도 경험했던 분야라 일정도 어느정도 예상할수 있었고 진행에 큰 무리가 없을거라 생각했다.\n문제는 프론트엔드\u0026hellip; 간단하게는 그냥 fullstack Framework를 사용하면 된다.(Django면 Template이나 Jinja) 하지만 예전에 경험해봤을때, 규모가 커지면 개발이 너무 복잡해져 힘들었다. 그래서 이번에는 React를 사용하여 한번 개발해보기로 결정하였다(개인적인 욕심). React 경험이 없어, 처음에는 러닝커브가 어느정도 있을거라 예상했지만 어느정도 익숙해지면 오히려 속도가 더 나올것이라 예상했다. 이렇게 1인 개발 여정이 시작되었다.\nTech Stack 인프라 : AWS, IaC(terraform), CI/CD(github action, argocd), Docker, k8s\n백엔드 : Python(django), Node + Typescript(express)\n프론트엔드 : React + Typescript 실제 구성 구성한 인프라: 좀 더 자세한 구성은 위 그림과 같이 하였다. 프론트엔드 부분 구성과 배포는 AWS amplify로 구축하였다. AWS cognito(인증)와 같이 기타 AWS 서비스 연결과 자동배포, 호스팅등 간단하게 풀스택 세팅할 수 있기 때문이다. (백엔드의 경우는 amplify를 사용하지 않았다.) 인증의 경우 위에 적었듯이 AWS cognito를 사용하였으며 Frontend에서 각 API에 접근하기 위해 AWS ALB path routing을 세팅하였다.\n결과물 3월부터 팀에 참가해서 4월부터 본격적으로 개발을 시작하였다. 결론적으로는 9월 중순에 (총 6개월 정도 소요)해서 요구한 기능들을 모두 개발하여 내부테스트까지 진행하였다. 개발한 내용은 프론트엔드 + 4개의 백엔드 모듈(비즈니스 로직 관련 API, SNS 인증 관련 API, 기타 알림이나 주소관련 API, 채팅 API)이다. 하지만 회사 사정으로 앱 출시까지는 하지 못하고 프로젝트를 종료하게 되었다. (너무 아쉬운 상황이 되어버렸다.)\n느낀점 좋은 점은 일단 혼자서 인프라 구축부터 프론트엔드, 백엔드 개발까지 혼자 경험해본 것이 매우 귀중한 경험이었다고 생각한다. 실제 인증은 어떤 Flow로 진행되는지, 프론트엔드와 백엔드 사이에서 데이터를 주고받을때 어떻게 해야 효율적인지? 여러가지를 고민을 해볼 수 있는 시간이었다.(백엔드만 개발했을때는 고민해보지 못한 그런\u0026hellip;?) 초기 인프라 및 CI/CD 자동화를 세팅하였기 때문에 그 뒤로는 개발에만 집중할 수 있었서 생각보다는 많이 힘들지는 않았다. 혼자 진행하다보니 팀으로 진행했을때 생기는 여러가지 상황들이 없기 때문이었을 것이다.(코드리뷰라던지 그 밖의 많은 이슈들) 물론 프론트엔드 React를 개발하면서 많은 난관이 있었다. 처음 해본 언어였기 때문일수도 있고, html 에 대한 이해도 부족했고 마크업도 외주로 진행하다보니 제대로 된 지원을 받기 어려웠다. 그리고 실제 사용자가 사용하는 화면을 개발하는것이라 그런지 꼼꼼하게 체크할(예외처리나 편의성 부분)부분이 많이 있어 생각했던 것 보다 작업량이 많았다.\n아쉬웠던 점은 아무래도 코드 퀄리티에 많은 신경을 쓰기 힘들다는 점이다. 혼자서 많은 양을 정해진 일정 안에 개발해야 하기 때문에 점점 마음에 들지 않는 코딩을 하게 되었다. 그리고 내부에서 조언을 구하거나 도움을 받을 팀원이 없다는 점이었다. 정신없이 코딩하게 되면 어느 순간에 삽질의 순간에 빠지게 되는데, 이때 해결되지 않으면 멘붕의 순간에 빠진다. 지나고 보면 별것도 아닌데 그 순간에 누군가 같이 고민해줬다면 훨씬 빠른시간 안에 해결할 수 있었을 것이라 생각한다. 같이 성장할 수 있는 팀원이 옆에 있다는게 얼마나 소중한 경험인지 다시 한번 깨닫게 되었다.\n약 7개월간의 1인 개발여정을 짧은글로 정리하면서 스스로에 대한 뿌듯함이 들기도 하고, 많이 힘들었던 기억도 지나가는 것 같다. 이번 경험이 좋은 밑거름이 되길 바라며 이만\u0026hellip;\nps. 스타트업이면 AWS activate 신청하세요. (기본 1,000$에 설문조사도 하면 추가로 300$ 지원받을수있음)\n","description":"","id":5,"section":"posts","tags":["1인 개발","AWS activate","풀스택"],"title":"1인 개발로 서비스 런칭하며...","uri":"https://Shin0102.github.io/posts/indie-developer/"},{"content":"고려해야 될점 Service Discovery : Container가 Scailing 되어도 해당서비스를 찾을 수 있어야 함 API Gateway : 외부에서 내부의 서비스들에 접근에 대한 제어(인증, 로깅, 모니터링, Response) Load balance : Container나 Instance 성능을 위해 외부요청을 적절히 분배 Cirecuit Breaker : 하나의 서비스 장애가 전체 서비스의 장애가 되지 않아야함 IPC : 서비스 사이의 통신 Sync : HTTP, gRPC Async : RabbitMQ, AWS SQS, Kafka, Kinesis 등등 DDD (Domain Drive Design -\u0026gt; Event storming, Bounded context) : MSA를 어떻게 구성할지에 대한 방법론 MSA Pattern SAGA : MSA에서의 트랜잭션 처리 Choreography-Based Saga Orchestrations-Based Saga Event Source : 발생하는 모든 event를 event table의 저장 CQRS : CUD와 Read를 분리(AWS DynamoDB와 같은 DB와 sync), Event Source 패턴과 결합 ","description":"","id":6,"section":"posts","tags":["MSA"],"title":"MSA 설계시 고려해야될 점과 Pattern","uri":"https://Shin0102.github.io/posts/devops-msa-considerations/"},{"content":"들어가며 작년부터 회사의 거대한 레거시 시스템을 Python 기반의 API 서버로 조금씩 포팅하고 있다. 한번에 모두 포팅하기란 불가능하기에 MSA를 기반으로 여러 기능들로 나누어(메시지, 결제, 고객용 API 등등\u0026hellip;) 포팅을 진행하였다. 하지만 처음에 배포환경이나 Framework 대한 결정없이 각각 진행하다보니, AWS EC2, Lambda, ECS등 다양한 환경에서 API 서버들이 돌아갔다. 그러다 보니 해당 모듈의 담당자가 없으면 서비스를 배포하기가 힘든 환경이 되었고, 배포를 위해서 노트북마다 환경에 맞는 세팅이 필요하게 되었다. 이러한 상황을 극복하기 위해 배포환경을 통일하는게 필요하였고, 그래서 아래와 같이 EKS 기반의 배포 자동화 파이프라인을 구축하였다.\nKubernetes 배포 자동화 배포 자동화를 위한 EKS pipeline: 참고. MSA를 위한 Kubernetes 세팅과 CI/CD Pipeline 구성\n구성하는데 위 블로그의 도움을 많이 받았으며, 크게 다른점은 Jenkins 대신에 CDK로 AWS CodePipeline을 생성하여 사용하였다. 간단히 Flow를 설명하면 아래와 같다.\nflow 1. Code commit\n2. AWS CodePipeline에서 Code commit 이벤트를 받아 Docker Build 하고 ECR에 해당 이미지를 Push\n3. AWS CodePipeline에서 GitOps repository에 있는 helm values file을 업데이트 (이미지 태그나 secret value 등등)\n4. AWS CodePipeline \u0026lsquo;success\u0026rsquo; 또는 \u0026lsquo;fail\u0026rsquo;을 슬랙으로 알림\n5. argocd 에서 Girpops repository의 변경내역이 발생하면, 변경된 내용을 기반으로 자동(or 수동)으로 Kubernetes내의 Resource를 업데이트 한다.\n실제 운영중인 gui: 실제로 위와 같이 배포 파이프라인을 구축하고 난 뒤에는 배포에 대한 부담이 많이 줄어들었다. 배포를 위한 다른 세팅이나 Kubernetes Cluster 접근할 필요없이 argocd gui를 통해 배포 \u0026amp; 롤백을 매우 편하게 할 수 있게 되었기 때문이다. 하지만 프로젝트를 새로 생성 할때마다, AWS CodePipeline을 추가를 해줘야하는 수고스러움?이 발생한다. 물론 CDK를 이용하여 이것도 자동화를 하였지만, 매번 AWS CodePipeline을 생성해야 된다는 것이 그렇게 반가운 상황은 아니었다. 그렇게 고민하던중 github action을 알게 되었고, 파이프라인 구축을 좀 더 간소화 할 수 있을것 같은 기대감을 가지고 세팅을 시작해보았다.(github action을 써보고 싶기도 하였다..)\ngithub action github action이란 github에서 제공하는 CI/CD 툴이다. 사용법은 간단하다. gitub project에 Actions 탭에 들어가면 새로운 workflow를 만들 수 있다. yml 파일로 만들어지며, 생성된 파일은 default로 .github/workflows/{name}.yml 위치하게 된다. 자세한 설명은 잘 정리해주신 블로거가 많으니 참고하면 될 것 같다. 일단 해야될 work을 정해보면 아래와 같다.\ndocker build AWS ECR push GitOps repository의 Kubernetes deployment.yaml image tag를 update GitOps repostroy의 Kubernetes deploymnet.yaml의 image tag에 빌드된 이미지의 태그가 업데이트 되면, GitOps repository와 연동해놓은 argocd 에서 자동(or 수동)으로 Cluster에 해당하는 Pod를 업데이트 하여 sync를 맞춘다. 일단 위 작업들을 정의한 github action은 아래와 같다.\nproject name : 테스트라 가정. gitops repo 아래 경로에 k8s yaml(service, deployment)이 정의되어있다고 가정. gitops repo/dev/테스트/resources.yaml gitops repo/prod/테스트/resources.yaml (github action을 수행하기전에 아래 값들을 github secrets에 세팅필요)\nGithub Secrets AWS_ACCESS_KEY_ID_VAL AWS_SECRET_ACCESS_KEY_VAL ECR_REPOSITORY -\u0026gt; AWS ECR Repository 이름 GIT_ACCESS_TOKEN -\u0026gt; gitops git repsitory를 checkout 하기위한 Token 예제 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 on: push: branches: - master - feature name: Docker Image build \u0026amp; k8s resources.yaml update jobs: deploy: name: Deploy runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - name: Set Dev env variables if: endsWith(github.ref, \u0026#39;/feature\u0026#39;) run: | echo \u0026#34;Feature branch\u0026#34; echo \u0026#34;ENVIRONMENT=dev\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV echo \u0026#34;DESTINATION=dev/테스트\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV echo \u0026#34;BRANCH=feature\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV - name: Set Prod env variables if: endsWith(github.ref, \u0026#39;/master\u0026#39;) run: | echo \u0026#34;Master branch\u0026#34; echo \u0026#34;ENVIRONMENT=prod\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV echo \u0026#34;DESTINATION=prod/테스트\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV echo \u0026#34;BRANCH=master\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV - name: Set commit msg env: GITHUB_SHA: ${{ github.sha }} run: | echo \u0026#34;COMMIT_MSG=Update from https://github.com/your/테스트/$GITHUB_SHA\u0026#34; \u0026gt;\u0026gt; $GITHUB_ENV echo $COMMIT_MSG # # Push app image to ECR # - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v1 with: aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID_VAL }} aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY_VAL }} aws-region: ap-northeast-2 - name: Login to Amazon ECR id: login-ecr uses: aws-actions/amazon-ecr-login@v1 - name: Build, tag, and push image to Amazon ECR id: build-image env: ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }} ECR_REPOSITORY: ${{ secrets.ECR_REPOSITORY }} IMAGE_TAG: ${{ github.sha }} run: | docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$ENVIRONMENT-$IMAGE_TAG {Dockerfile 경로} docker push $ECR_REGISTRY/$ECR_REPOSITORY:$ENVIRONMENT-$IMAGE_TAG echo \u0026#34;::set-output name=image::$ECR_REGISTRY/$ECR_REPOSITORY:$ENVIRONMENT-$IMAGE_TAG\u0026#34; # # Update Image tag to delployment.yaml # - name: Git config run: | git config --global user.email \u0026#34;actions@github.com\u0026#34; git config --global user.name \u0026#34;GitHub Actions\u0026#34; - name: Check out k8s repo uses: actions/checkout@master with: repository: your/gitops-repository token: ${{ secrets.GIT_ACCESS_TOKEN }} - name: chmod repo destination run: | chmod +x $DESTINATION ls - name: git push image tag to k8 repo env: GITHUB_REPO: ${{ github.repository.name }} GITHUB_SHA: ${{ github.sha }} run: | sed -i -e \u0026#39;s@${{ steps.login-ecr.outputs.registry }}/${{ secrets.ECR_REPOSITORY }}:.*@${{ steps.build-image.outputs.image }}@g\u0026#39; $DESTINATION/resources.yaml git add . git commit -m \u0026#34;$COMMIT_MSG\u0026#34; git push -f --set-upstream origin main 구조를 보면 트리거 / build 환경 / 실제 수행할 step들이 정의되어 있다. feature, master branch 에 push될때 dev와 production 서버를 업데이트 하기위한 github action이다. 라인별 설명은 아래와 같다.\nline 19~33 : GitOps repository에 해당 프로젝트의 kubernetes 리소스들이 있는 path를 지정해준다.(dev, prod)\nline 39 : GitOps에 commit할 msg 지정 -\u0026gt; 현재 commit의 url link를 commit msg로 하기위해 repository url을 세팅해준다.\nline 55~64 : docker build하고 지정한 ECR에 push\nline 74~78 : GitOps repository checkout\nline 80~83 : 실행권한 추가\nline 85~93 : k8s deployment가 정의된 yaml 파일의 tag를 업데이트\nAWS CodePipeline과 비교 일단 기존의 사용하던 codepipe line과 비교해보면 사용법이 굉장히 쉬웠다. AWS Codepipeline도 iac tool 을 이용해서 쉽게 세팅할 수 있지만, 초기 세팅이 쉽지는 않았다. 그리고 github action이 좋았던 것이 브랜치별로 pipeline을 구성할 필요가 없다는 것이 굉장히 편리했다. step에서 조건문을 이용하면 branch 별로 action을 제어할 수 있다. 마지막으로 빌드 시간에서 생각보다 많은 차이가 났다. 기존 CodePipeline을 사용했을때는 5~6분정도 소요 되었는데 github action은 절반도 안되는 시간이(2분 정도) 소요되었다.\n참고. GitHub Actions vs. AWS CodePipeline\n마치며 AWS CodePipeline에서 github action으로 cd pipeline을 변경하면서 구성이 굉장히 가벼워진 느낌이 들었다. 프로젝트 내에서 branch별로 pipeline을 만들 필요도 없고, 간단히 프로젝트 repository에 workflow가 정의된 yml파일만 추가해주면 되기 때문이다. 결과적으로는 github을 repository로 사용하고 있고, AWS Resource에 의존성이 크지 않으면 github action을 사용하지 않을 이유는 없을 것 같다.\n","description":"","id":7,"section":"posts","tags":["github action","Kubernetes","argocd"],"title":"github action으로 Kubernetes 배포 자동화","uri":"https://Shin0102.github.io/posts/devops-github-action/"},{"content":" MSA(microservice architecture)를 위해 CDK(typescript)를 통해 EKS를 구성한 적이 있는데, 이때는 동작에만 집중하다 보니, 자동으로 생성되는 리소스나 Role들에 대해 크게 신경 쓰지 않았다. 그러다 이번에 번잡하게 관리되는 AWS Resource들에 대해 정리할 기회가 생겨, 새로 EKS 환경을 구성할 기회가 생겼다. 그중에 이번에는 Role에 대해 정리해보려 한다.\nCDK를 통해 eks를 생성하게 되면 role에 대해 특별히 명시하지 않아도, 자동으로 IAM Role들이 몇 개 만들어진다. Cluster에 대한 Role, 그리고 같이 생성되는 Lambda에 대한 Role도 여러 개 있다.(Lambda 관련된 것들은 다음에 정리해봐야겠다) 먼저 Cluster 관련 Role을 정리해 보면 아래와 같다.\nKubernetes v1.18, CDK 1.78.0/ 세팅에 따라 이름이나, Role 개수가 다를 수 있음\n1. {cluster-name}-ClusterCreationRole\n2. {cluster-name}-admin-role\n3. {cluster-name}-eks-role\n4. {cluster-name}-ClusterNodegroupNodesNodeGroupRole 1. {cluster-name}-ClusterCreationRole 먼저 {cluster-name}-ClusterCreationRole을 살펴보면, ClusterCreationRoleDefaultPolicy라는 인라인 Policy(이것도 자동으로 생성됨)가 연결되어있다. 정책 내용에는 EKS Cluster를 생성에 사용할 Subnet과 VPC에 대한 정보에 대한 접근 허용, Cluster 생성/삭제/버전업데이트 등 클러스터 생성에 관한 권한들이 나열되어 있다. 내용이나 이름으로 보아 cdk에서 cluster 생성 및 관리할때 사용하는 Role 이다.\n2. {cluster-name}-admin-role {cluster-name}-admin-role은 확인해보면 아무런 Policy도 연결되어 있지 않다. 그래서 좀 찾아보니 먼저 EKS인증 방식을 알아야 한다.\nAWS 인증방식: https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/managing-auth.html kubectl 을 통해 Cluster에 접근하게 되면 해당 유저가 적절한 I AM User인지 확인한 후, 인증되면 k8s(Kubernetes)의 RBAC을 이용해 접근제어를 하게 된다. {cluster-name}-admin-role은 Cluster를 초기 생성하는 User에게 system:masters라는 그룹을 부여하게 된다. 이 그룹은 k8s Default ClusterRole인 cluster-admin(super user)이라는 ClusterRole과 바인딩되어 있다. 그래서 Cluster 초기 생성자는 Cluster관한 거의 모든 권한을 가지고 있다.\n참고. k8s default cluster role and role bidings 3. {cluster-name}-eks-role {cluster-name}-eks-role은 AmazonEKSClusterPolicy라는 기존 Policy가 연결되어 있는데, 설명을 보면 아래와 같다.\nThis policy provides Kubernetes the permissions it requires to manage resources on your behalf. Kubernetes requires Ec2:CreateTags permissions to place identifying information on EC2 resources including but not limited to Instances, Security Groups, and Elastic Network Interfaces.\nk8s 에서 AWS 리소스들을 관리하기 위해 필요한 권한들이라는 설명이다. Policy를 확인해보면 EC2, Load Balancer, AutoscalingGroup 등 서비스 운영에 주요한 리소스들에 대한 권한들이 포함되어 있다. AWS Console에 들어가보면 생성한 Cluster에 해당 Role이 연결되어 있는것을 확인할 수 있다.\n4. {cluster-name}-ClusterNodegroupNodesNodeGroupRole 마지막으로 {{cluster-name}-ClusterNodegroupNodesNodeGroupRole을 확인해보면, 총 3가지의 AWS Policy가 연결되어있다.\nAmazonEKSWorkerNodePolicy : EKS Worker node를 EKS Cluster에 연결하기 위한 Policy AmazonEC2ContainerRegistryReadOnly : ECR(컨테이너 저장소) Readonly Policy AmazonEKS_CNI_Policy : EKS에서 Conatiner 들의 Network Interface들을 관리하기 위해 EKS는 AWS CNI라는 plugin을 사용하는데, 해당 플러그인에게 제공하는 Policy 확인해 보니 Container Image를 읽어와 Worker Node안에 Pod들을 배포할때 필요한 정책들이 들어가 있다. 실제로 Worker Node들은 EC2 Instance에 해당하고, Pod들은 이 Worker Node들 안에 적절히? 배포된다.\n이제 Role들에 대해 알아봤으니, 실제 구성한 환경에서 확인해보자. 아래 명령어로 eks 생성시 만들어진 aws-auth configmap을 확인할 수 있다. aws-auth configmap을 수정을 통해 클러스터에 접근하는 유저나 Role을 관리할 수 있으며, 아래 명령어를 통해 수정하거나 yaml을 통해 적용가능하다.\n1 kubectl edit -n kube-system configmap/aws-auth aws auth configmap 처음 eks를 생성하면 aws configmap이 위와 같이 정의되어 있다. (실제로 String 형태로 되어있었지만 가독성을 위해 위 형식으로 수정하였다.). 위에서 설명한 Workder Node관련 Role과 Cluster 초기 생성자와 관련된 Role이 보인다.\nmapUsers에는 원래 아무것도 없었지만, EKS 생성후에 AWS Console EKS에서 Nodegroup에 대한 정보가 제대로 나오지않고 \u0026lsquo;Your current user or role does not have access to Kubernetes objects on this EKS cluster\u0026rsquo; 에러가 나와서 로그인한 I Am user에 master 권한을 추가해주니 정상적으로 Nodegroup 정보가 출력되었다. 다른유저들이 EKS Cluster에 접근하게 추가하고싶으면 mapUsers부분에 추가해주면 된다.\n이글은 아래 내용을 많이 참고하였고, EKS 환경을 구성하게 된다면 꼭 한번 보기를 추천한다. Custom Role과 Group을 만드는 내용도 있어 다음에 필요하면 세팅해볼 예정이다.\n쿠알못이 Amazon EKS로 안정적인 서비스 운영하기\n","description":"","id":8,"section":"posts","tags":["aws eks","cdk","role","iam"],"title":"AWS EKS Role에 대한 고찰","uri":"https://Shin0102.github.io/posts/devops-aws-eks-role/"},{"content":"hugo zzo 테마에서 사용 가능한 shortcodes 기본적으로 Markdown 문법도 제공하지만, 아래 shortcode들도 사용가능해서 블로그 쓰는데 유용할 것 같다. 이외에도 flowchartjs등 차트라이브러리도 사용가능하다.\nthis is a text this is a text this is a text this is a text Expand me Some Markdown Contents Image4: Image description success color Some contents Some markdown contents Windows MacOS Ubuntu Windows section 1 console.log(\u0026#39;Hello World!\u0026#39;); MacOS section Hello world! Ubuntu section Great! emoji-cheat-sheet I ❤️ 🐟\nzzo theme document\n","description":"","id":9,"section":"posts","tags":["zzo theme"],"title":"zzo theme shortcodes","uri":"https://Shin0102.github.io/posts/etc-zzo-theme-shotcodes/"},{"content":" Experience Python(Django, FastAPI, Flask) Backend Developer\nAWS, EKS, Serverless Devops\nMigration legacy services to microservices\nInterst Full stack Developer(Learning React)\nDeep dive devops\nGolang\n","description":"Alli about page","id":10,"section":"","tags":null,"title":"About","uri":"https://Shin0102.github.io/about/"}]